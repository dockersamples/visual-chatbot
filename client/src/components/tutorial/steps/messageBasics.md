# Message basics

This may be a "hot take", but here goes:

> [!warning] Hot take!
> Creating LLM-based applications isn't _that_ different than any other application.

Why? Well, you're simply interacting with an API. Requests go in. Some LLM magic happens. And responses come back out. The only difference is the responses are _very_ likely to be different every time, even with the same inputs.

For this training, we're going to focus on chat-based interactions, so let's look at its API.

## The chat API endpoint

One of the things OpenAI did is they created an API specification around their LLM. Many of the other LLM providers are now following this schema. 

The `/v1/chats/completion` endpoint ([full docs here](https://platform.openai.com/docs/api-reference/chat/create)) requires two fields: `messages` and `model`:

- `model`: your intended model
- `messages`: a collection of the messages for the conversation up to this point (more on that shortly)

Here's a quick example:

```json with-copy highlight=6-9
curl -v {{ENDPOINT}} \
    -H "Content-type: application/json" \
    -X POST --data-raw '
{
  "model": "{{MODEL}}",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant. Blah blah..." },
    { "role": "user", "content": "Hello!" }
  ]
}'
```

This goes to the LLM and generates a response. The response will contain quite a few things, but the `choices` element is the most important:

```json highlight=9-12
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-4o-mini",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "\n\nHello there, how may I assist you today?",
    },
    "logprobs": null,
    "finish_reason": "stop"
  }],
  "service_tier": "default",
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  }
}
```

You'll notice in the response's `choices` array, there is another message! But, this one has a different `role` associated with it.

### Message roles

Think of the "role" of a message similar to the type of the message. A few of the roles are outlined below:

- **system** - information on how the system should operate. What are the rules of engagement? How should the LLM structure responses? What are things it must or must never do?
- **user** - messages sent by the user. These are typically the messages typed in the chat interaction or through other mechanisms.
- **assistant** - messages generated by the AI assistant (like we see in the response)


## Your task

Close this tutorial and interact with the LLM by submitting messages. You can click on any message to view more details about the message, including its role and content.

As a bonus, change the System Prompt (by going into **Settings** -> **System prompt**) to a different character and see how the responses vary.